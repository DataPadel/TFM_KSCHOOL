{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "1. Using Pre-trained Image Models for Embeddings**\n",
    "Platforms like Hugging Face provide access to pre-trained vision models (e.g., ResNet, Vision Transformers, CLIP). These models can extract embeddings (numerical representations) from images, capturing features like posture, swing style, or environment.\n",
    "\n",
    "- Pre-trained Models to Conside:\n",
    "  - CLIP*: Combines image and text understanding, enabling semantic embeddings.\n",
    "  - *ResNet/VGG**: Traditional deep learning models for visual feature extraction.\n",
    "  - Vision Transformers (ViT): State-of-the-art models for capturing global image features.\n",
    "\n",
    "- Steps**:\n",
    "  1. **Select a Pre-trained Model**: Look for models fine-tuned on sports or action datasets if possible.\n",
    "  2. **Generate Embeddings**:\n",
    "     - Pass client-uploaded images through the pre-trained model to extract embeddings.\n",
    "     - Store these embeddings for downstream tasks.\n",
    "\n",
    "---\n",
    "\n",
    "2. Mapping Image Embeddings to Racket Recommendations**\n",
    "Once you have embeddings, the next step is to map these representations to racket recommendations:\n",
    "\n",
    "#Option A: Cluster-Based Mapping**\n",
    "Use clustering algorithms (e.g., K-Means or DBSCAN) to group embeddings based on playing styles.\n",
    "\n",
    "- Assign rackets to clusters based on their suitability for each style.\n",
    "- Recommend rackets corresponding to the closest cluster of the client’s image embedding.\n",
    "\n",
    "#Option B: Train a Mapping Model**\n",
    "Train a simple model to predict racket suitability from embeddings.\n",
    "\n",
    "- **Input**: Image embeddings.\n",
    "- **Output**: Racket features (e.g., weight, balance) or direct recommendations.\n",
    "- **Model Options**: Gradient Boosting (e.g., XGBoost), or a lightweight neural network.\n",
    "\n",
    "---\n",
    "\n",
    "3. Integrating Multi-Modal Recommendations**\n",
    "To combine image-based insights with your existing survey and racket dataset:\n",
    "\n",
    "- **Multi-Modal Fusion**:\n",
    "  - Combine survey data embeddings, image embeddings, and racket features.\n",
    "  - Use a multi-modal deep learning model (e.g., Wide & Deep, or a custom architecture).\n",
    "\n",
    "- **Hybrid Recommendation**:\n",
    "  - Use the image model to refine recommendations from survey-based models.\n",
    "  - Adjust rankings based on style analysis.\n",
    "\n",
    "---\n",
    "\n",
    "4. Using Hugging Face Models**\n",
    "Hugging Face offers a seamless way to integrate pre-trained models:\n",
    "1. **Search for Models**: Visit the [Hugging Face Model Hub](https://huggingface.co/models) and search for vision models suited to sports or human posture analysis.\n",
    "2. **Integration**:\n",
    "   - Use `transformers` or `datasets` libraries to load and use the model.\n",
    "   - Example (CLIP Model):\n",
    "\n",
    "```python\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load and preprocess image\n",
    "image = Image.open(\"player_image.jpg\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\", do_center_crop=True)\n",
    "\n",
    "# Extract embeddings\n",
    "outputs = model.get_image_features(**inputs)\n",
    "embeddings = outputs.detach().numpy()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "5. Evaluation and Feedback Loop**\n",
    "- Collect feedback from clients to refine recommendations.\n",
    "- Evaluate using metrics such as:\n",
    "  - **Image Similarity**: Use cosine similarity between embeddings.\n",
    "  - **User Satisfaction**: Direct feedback from recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (4.48.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ximef\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nCLIPModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load model and processor\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m processor \u001b[38;5;241m=\u001b[39m CLIPProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load and preprocess image\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:1690\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1690\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:1678\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1676\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1678\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nCLIPModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load and preprocess image\n",
    "image = Image.open(\"player_image.jpg\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\", do_center_crop=True)\n",
    "\n",
    "# Extract embeddings\n",
    "outputs = model.get_image_features(**inputs)\n",
    "embeddings = outputs.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sure! Let me break it down step-by-step and explain how Eimage embeddings** work and how you can use them for your padel recommendation system.\n",
    "\n",
    "---\n",
    "\n",
    "### **What Are Embeddings?**\n",
    "- **Embeddings** are numerical representations of data, such as text, images, or audio, in a high-dimensional vector space.\n",
    "- For images, embeddings capture the visual features of an image (e.g., shapes, colors, patterns, and relationships between objects). These features are encoded as a vector of numbers.\n",
    "\n",
    "Think of an embedding as a \"compressed summary\" of an image that allows a machine learning model to understand and compare images.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Use Image Embeddings in Your Case**\n",
    "You want to recommend padel rackets based on client-uploaded photos of them playing. The key steps are:\n",
    "\n",
    "#### 1. **Extracting Embeddings**\n",
    "   - Use a pre-trained image model (like CLIP or ResNet) to process the uploaded photo.\n",
    "   - The model analyzes the image and outputs a vector of numbers (the embedding) that represents the player's style, posture, environment, or other visual features.\n",
    "\n",
    "   Example:\n",
    "   - Input: An image of a person playing padel.\n",
    "   - Output: A vector like `[0.23, -0.87, 1.56, ..., -0.45]`.\n",
    "\n",
    "#### 2. **Comparing Embeddings**\n",
    "   - Embeddings allow you to measure the similarity between images.\n",
    "   - If two images have similar embeddings, they represent similar visual styles or patterns.\n",
    "   - You can use metrics like **cosine similarity** or **Euclidean distance** to compare embeddings.\n",
    "\n",
    "   Example:\n",
    "   - Player's image embedding: `[0.23, -0.87, 1.56, ..., -0.45]`\n",
    "   - Another player's embedding: `[0.25, -0.85, 1.53, ..., -0.48]`\n",
    "   - The two embeddings are similar, so their playing styles might be alike.\n",
    "\n",
    "#### 3. **Mapping Embeddings to Rackets**\n",
    "   - Use a dataset of pre-labeled images of players with their preferred rackets (or approximate recommendations based on features like weight, balance, etc.).\n",
    "   - Cluster embeddings of these reference images and associate each cluster with a type of racket.\n",
    "\n",
    "   Example:\n",
    "   - Cluster A (lightweight rackets): Embeddings of players with a defensive playing style.\n",
    "   - Cluster B (balanced rackets): Embeddings of players with an all-around style.\n",
    "   - Cluster C (power rackets): Embeddings of players with aggressive play.\n",
    "\n",
    "   When a new player uploads an image:\n",
    "   - Extract their embedding.\n",
    "   - Find the closest cluster of reference embeddings.\n",
    "   - Recommend rackets associated with that cluster.\n",
    "\n",
    "#### 4. **Pre-trained Models**\n",
    "   - Use pre-trained models like **CLIP** or **ResNet** for embedding extraction.\n",
    "   - These models already understand visual features well and don’t require training from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It All Fits Together**\n",
    "1. **Client Uploads Image**:\n",
    "   - A player uploads a photo of them playing padel.\n",
    "\n",
    "2. **Extract Embedding**:\n",
    "   - Pass the image through a pre-trained model to extract the embedding.\n",
    "\n",
    "3. **Find Similar Styles**:\n",
    "   - Compare the embedding to existing embeddings in your database.\n",
    "   - Identify the closest matches (similar playing styles).\n",
    "\n",
    "4. **Recommend a Racket**:\n",
    "   - Based on the matched style, recommend a racket (e.g., lightweight for defensive, power-oriented for aggressive).\n",
    "\n",
    "'''\n",
    "\n",
    "##Example in PracticeELet’s say you use the ELIP modelE\n",
    "\n",
    "#Extract Embedding\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load the player's image\n",
    "image = Image.open(\"C:\\Users\\ximef\\Downloads\\volea_250.jpg\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the embedding\n",
    "embedding = model.get_image_features(Enputs)\n",
    "```\n",
    "\n",
    "#Compare to Reference Embeddings:\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Reference embeddings of existing players (pre-computed)\n",
    "reference_embeddings = [\n",
    "    [0.21, -0.9, 1.5, ..., -0.5],  # Defensive\n",
    "    [0.3, -0.8, 1.4, ..., -0.6],  # Aggressive\n",
    "    [0.25, -0.85, 1.55, ..., -0.45],  # All-around\n",
    "]\n",
    "\n",
    "# Compare similarity\n",
    "similarities = cosine_similarity([embedding.numpy()], reference_embeddings)\n",
    "closest_cluster = similarities.argmax()\n",
    "\n",
    "# Map cluster to racket recommendation\n",
    "racket_recommendations = {\n",
    "    0: \"Lightweight Defensive Racket\",\n",
    "    1: \"Powerful Aggressive Racket\",\n",
    "    2: \"Balanced All-Around Racket\"\n",
    "}\n",
    "\n",
    "recommended_racket = racket_recommendations[closest_cluster]\n",
    "print(recommended_racket)\n",
    "\n",
    "### Ehy Use Pre-trained Models?E- Saves time and computational resources.\n",
    "- Extracts powerful features without requiring labeled padel images.\n",
    "- Can be easily integrated with your existing recommendation system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==2.5.0 (from versions: none)\n",
      "ERROR: No matching distribution found for torch==2.5.0\n"
     ]
    }
   ],
   "source": [
    "# CUDA 11.8\n",
    "%pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 torch-cuda==11.8 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nCLIPModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load model and processor\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m processor \u001b[38;5;241m=\u001b[39m CLIPProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the player's image\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:1690\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1690\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:1678\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1676\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1678\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nCLIPModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load the player's image\n",
    "image = Image.open(\"volea_250.jpg\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the embedding\n",
    "embedding = model.get_image_features(**inputs)\n",
    "\n",
    "#### b. **Compare to Reference Embeddings**:\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Reference embeddings of existing players (pre-computed)\n",
    "print(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_embeddings = [\n",
    "    [0.21, -0.9, 1.5, ..., -0.5],  # Defensive\n",
    "    [0.3, -0.8, 1.4, ..., -0.6],  # Aggressive\n",
    "    [0.25, -0.85, 1.55, ..., -0.45],  # All-around\n",
    "]\n",
    "\n",
    "# Compare similarity\n",
    "similarities = cosine_similarity([embedding.detach().numpy()], reference_embeddings)\n",
    "closest_cluster = similarities.argmax()\n",
    "\n",
    "# Map cluster to racket recommendation\n",
    "racket_recommendations = {\n",
    "    0: \"Lightweight Defensive Racket\",\n",
    "    1: \"Powerful Aggressive Racket\",\n",
    "    2: \"Balanced All-Around Racket\"\n",
    "}\n",
    "\n",
    "recommended_racket = racket_recommendations[closest_cluster]\n",
    "print(recommended_racket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall torch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\ximef\\AppData\\Local\\Temp\\ipykernel_16552\\3488153402.py:14: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  image_folder = \"Desktop\\MASTER DATA SCIENCE\\TFM\\1STEP. IMAGE COLLECTION\\BANDEJA\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: Pillow in c:\\users\\ximef\\appdata\\roaming\\python\\python313\\site-packages (11.1.0)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nCLIPModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load model and processor\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m processor \u001b[38;5;241m=\u001b[39m CLIPProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Define the folder containing images\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:1690\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1690\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:1678\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1676\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1678\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nCLIPModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install Pillow\n",
    "#import pytorch\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Load model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Define the folder containing images\n",
    "image_folder = \"Desktop\\MASTER DATA SCIENCE\\TFM\\1STEP. IMAGE COLLECTION\\BANDEJA\"\n",
    "output_embeddings_file = \"image_embeddings.pt\"  # File to save embeddings\n",
    "\n",
    "# Initialize a list to store embeddings\n",
    "image_embeddings = []\n",
    "image_names = []\n",
    "\n",
    "# Loop through the folder\n",
    "for file_name in os.listdir(image_folder):\n",
    "    file_path = os.path.join(image_folder, file_name)\n",
    "    \n",
    "    try:\n",
    "        # Open and process the image\n",
    "        image = Image.open(file_path).convert(\"RGB\")  # Ensure 3 channels (RGB)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate and normalize the embedding\n",
    "        embedding = model.get_image_features(**inputs)\n",
    "        normalized_embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Store the embedding and image name\n",
    "        image_embeddings.append(normalized_embedding.detach().cpu())\n",
    "        image_names.append(file_name)\n",
    "        \n",
    "        print(f\"Processed {file_name}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# Combine embeddings into a single tensor\n",
    "image_embeddings = torch.cat(image_embeddings, dim=0)\n",
    "\n",
    "# Save embeddings and names\n",
    "torch.save({\"embeddings\": image_embeddings, \"names\": image_names}, output_embeddings_file)\n",
    "print(f\"Saved embeddings to {output_embeddings_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
